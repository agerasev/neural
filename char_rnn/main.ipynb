{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import random\n",
    "import time\n",
    "import signal\n",
    "\n",
    "import scipy.ndimage as nd\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n",
      "5088376 chars, 140 unique\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\n",
    "ddir = \"../data/books/witcher_rus/\"\n",
    "for fn in sorted(os.listdir(ddir)):\n",
    "    if fn.endswith(\".gz\"):\n",
    "        with gzip.open(ddir + fn, \"rb\") as f:\n",
    "            data += f.read().decode(\"utf-8\").replace(\"\\r\", \"\")\n",
    "charset = sorted(set(data))\n",
    "chidx = {ch: i for i, ch in enumerate(charset)}\n",
    "chvec = {ch: np.array([chidx[ch] == i for i in range(len(charset))], dtype=np.float32) for ch in charset}\n",
    "print(charset)\n",
    "print(\"%s chars, %s unique\" % (len(data), len(charset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_deriv(x):\n",
    "        return 1/np.cosh(x)**2\n",
    "\n",
    "def softmax(y):\n",
    "    #y = y - np.max(y)\n",
    "    ey = np.exp(y)\n",
    "    return ey/np.sum(ey)\n",
    "\n",
    "def cross_entropy_loss(y, ai):\n",
    "    return -np.log(y[ai])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19136775 0.42589676 0.19136775 0.19136775]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.1, 0.9, 0.1, 0.1])\n",
    "print(softmax(x))\n",
    "np.random.choice(['a', 'b', 'c', 'd'], p=softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell:\n",
    "    def __init__(self, sx, sh, sy, mag=1e-1):\n",
    "        self.sizes = (sx, sh, sy)\n",
    "        layer_sizes = [(sx, sh), (sh, sh), (sh, sy)] # W_xh, W_hh, W_hy\n",
    "        self.params = [ mag*np.random.randn(*s) for s in layer_sizes]\n",
    "    \n",
    "    def step(self, h, x, mem=False):\n",
    "        W_xh, W_hh, W_hy = self.params\n",
    "        v = np.dot(x, W_xh) + np.dot(h, W_hh)\n",
    "        a = np.tanh(v)\n",
    "        y = np.dot(a, W_hy)\n",
    "        if mem:\n",
    "            m = [x, h, v, a, y]\n",
    "        else:\n",
    "            m = None\n",
    "        return a, softmax(y), m\n",
    "    \n",
    "    def newgrad(self):\n",
    "        return [np.zeros_like(v) for v in self.params]\n",
    "    \n",
    "    def newstate(self):\n",
    "        return np.zeros(self.sizes[1], dtype=np.float64)\n",
    "    \n",
    "    def backprop(self, grad, eh, m, ey):\n",
    "        W_xh, W_hh, W_hy = self.params\n",
    "        dW_xh, dW_hh, dW_hy = grad\n",
    "        x, h, v, a, y = m\n",
    "        \n",
    "        dW_hy += np.outer(a, ey)\n",
    "        ea = np.dot(W_hy, ey) + eh\n",
    "        ev = ea*tanh_deriv(v)\n",
    "        dW_xh += np.outer(x, ev)\n",
    "        dW_hh += np.outer(h, ev)\n",
    "        eh = np.dot(W_hh, ev)\n",
    "        \n",
    "        return eh\n",
    "    \n",
    "    def learn(self, grad, learning_rate, adagrad=None):\n",
    "        if adagrad is None:\n",
    "            for W, dW in zip(self.params, grad):\n",
    "                W -= learning_rate*dW\n",
    "        else:\n",
    "            for W, dW, aW in zip(self.params, grad, adagrad):\n",
    "                aW += dW**2\n",
    "                W -= learning_rate*dW/np.sqrt(aW + 1e-8)\n",
    "        # rmsprop: aW += rho*aW + (1 - rho)*grad**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26320309  0.91429123  0.31532077  0.29169201 -0.97312506 -0.86623378\n",
      " -0.73840531  0.30238288 -0.25954162 -0.23406041  1.56373461  1.87434848\n",
      "  1.97406212 -1.82394484  1.84878496 -0.75463792 -1.27107995 -2.1896793\n",
      "  0.31094416  1.50679702]\n",
      "[-0.11202765 -0.54942735  0.15565051  0.65898392 -0.3227755   0.07139345\n",
      " -0.38709182 -0.15130839  0.48390714  0.45629931]\n",
      "[0.22764417 0.21168232 0.1824541  0.17352255 0.20469685]\n"
     ]
    }
   ],
   "source": [
    "sx, sh, sy = 20, 10, 5\n",
    "cell = Cell(sx, sh, sy)\n",
    "x = np.random.randn(sx)\n",
    "h = cell.newstate()\n",
    "nsteps = 10\n",
    "\n",
    "mem = []\n",
    "for i in range(nsteps):\n",
    "    h, y, m = cell.step(h, x, mem=True)\n",
    "    mem.append(m)\n",
    "\n",
    "print(x)\n",
    "print(h)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00835738 -0.01145026  0.00694131 -0.00048273  0.00360324 -0.03403861\n",
      "  0.03760375  0.0349259  -0.01131075 -0.02413754]\n"
     ]
    }
   ],
   "source": [
    "nlearn = 1\n",
    "\n",
    "for i in range(nlearn): \n",
    "    grad = cell.newgrad()\n",
    "    eh = cell.newstate()\n",
    "    for m in reversed(mem):\n",
    "        #print(m)\n",
    "        eh = cell.backprop(grad, eh, m, y - [1, 0, 0, 0, 0])\n",
    "    #[np.clip(dW, -5, 5, out=dW) for dW in grad]\n",
    "    cell.learn(grad, 1e-3)\n",
    "    print(eh)\n",
    "    #print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Cell(len(charset), 100, len(charset))\n",
    "grad = net.newgrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 100\n",
    "rate = 1e-1\n",
    "\n",
    "loss = 0.0\n",
    "mem = []\n",
    "iepoch = 0\n",
    "pos = 0\n",
    "h = net.newstate()\n",
    "adagrad = net.newstate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 1130000/5088376: 2.257617982927423\n",
      "одируж. iреим трак тра жно булит, мот за породе же дложно. I8оне ем й евцель ва ботыы та обтантом ст\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "def signal_handler(signal, frame):\n",
    "    global done\n",
    "    done = True\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "while not done:\n",
    "    h, y, m = net.step(h, chvec[data[pos]], mem=True)\n",
    "    mem.append((m, y, data[pos + 1]))\n",
    "    loss = 0.99*loss + 0.01*cross_entropy_loss(y, chidx[data[pos + 1]])\n",
    "    \n",
    "    if (pos + 1) % seqlen == 0:\n",
    "        [dW.fill(0) for dW in grad]\n",
    "        eh = net.newstate()\n",
    "        for m, y, ch in reversed(mem):\n",
    "            eh = net.backprop(grad, eh, m, y - chvec[ch])\n",
    "        [np.clip(dW, -5, 5, out=dW) for dW in grad] # mitigate exploding gradient\n",
    "        net.learn(grad, rate/seqlen, adagrad=adagrad)\n",
    "        mem = []\n",
    "    \n",
    "    if pos % 10000 == 0:\n",
    "        clear_output()\n",
    "        print(\"%s - %s/%s: %s\" % (iepoch, pos, len(data), loss))\n",
    "        \n",
    "        ch = data[pos]\n",
    "        _h = h\n",
    "        for i in range(100):\n",
    "            _h, y, _ = net.step(_h, chvec[ch])\n",
    "            ch = np.random.choice(charset, p=y)\n",
    "            print(ch, end='')\n",
    "        print()\n",
    "        \n",
    "    pos += 1\n",
    "    if pos >= len(data) - 1:\n",
    "        pos = 0\n",
    "        iepoch += 1\n",
    "        \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
